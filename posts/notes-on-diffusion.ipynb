{
 "cells": [
  {
   "cell_type": "raw",
   "id": "53fd6df8",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Notes on diffusion\"\n",
    "date: 2022-10-01\n",
    "categories: [artificial intelligence]\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1d301",
   "metadata": {},
   "source": [
    "## Background: understanding the math in ML papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f109c6b",
   "metadata": {},
   "source": [
    "The most popular introductions to diffusion models derive them from starting points like non-equilibrium thermodynamics, or make assumptions that you're an expert in the generative modeling literature, probability theory, [Langevin dynamics](https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm), and so on. I don't have anything against these articles. In fact, I think they're excellent, they help me develop my understanding of diffusion models, and they're very well written. But there isn't really an article for people without these backgrounds.\n",
    "\n",
    "There isn't an introduction to diffusion models for coders. This is that introduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc495ac",
   "metadata": {},
   "source": [
    "I would go so far as to say that the math-first approach can hinder further development of the ideas. I think it hamstrings our thinking into forms that fit math (e.g. Gaussian distributions and so on) whereas history has shown that progress in deep learning is often made through leaps that aren't informed by math."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b735e15",
   "metadata": {},
   "source": [
    "There are two main ways to derive diffusion models.\n",
    "\n",
    "1. The first is to assume that you generate images by iteratively adding Gaussian noise. Then try to estimate the reverse process using variational inference (?).\n",
    "2. The second is to try to sample from the data distribution using the gradient of the log likelihood, so-called \"score-based\" models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f91eb",
   "metadata": {},
   "source": [
    "(I think Jeremy is developing another \"hacker\" approach to derive it as a natural consequence of GANs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1aea5b",
   "metadata": {},
   "source": [
    "## Deep Unsupervised Learning using Nonequilibrium Thermodynamics\n",
    "\n",
    "_Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015, June._\n",
    "\n",
    "https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models\n",
    "\n",
    "https://arxiv.org/abs/1503.03585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ff36e-6916-4f65-95a8-6867bf75026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2566b7c1",
   "metadata": {},
   "source": [
    "### 2.1. Forward Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a592a1",
   "metadata": {},
   "source": [
    "::: {.pt-3 .pb-1 .px-3 .my-3 .border .rounded .shadow-sm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12107faa",
   "metadata": {},
   "source": [
    "We label the [data distribution $q(\\mathbf{x}^{(0)})$]{.hl-indigo}. The data distribution is gradually converted into a [well behaved (analytically tractable) distribution $\\pi(\\mathbf y)$]{.hl-green} by repeated application of a [Markov diffusion kernel $T_\\pi(\\mathbf y|\\mathbf y'; \\beta)$]{.hl-yellow} for $\\pi(\\mathbf y)$, where [$\\beta$ is the diffusion rate]{.hl-blue},"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e577e1b4",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\pi(\\mathbf y) &= \\int d\\mathbf y' T_\\pi (\\mathbf y|\\mathbf y';\\beta) \\pi(\\mathbf y') \\tag{1} \\\\\n",
    "q(\\mathbf x^{(t)}|\\mathbf x^{(t-1)}) &= T_\\pi(\\mathbf x^{(t)} | \\mathbf x^{(t-1)}; \\beta_t) \\tag{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afecaadd-f8d5-4914-b482-af3f9f2630c8",
   "metadata": {},
   "source": [
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c8bdd9",
   "metadata": {},
   "source": [
    "There are four objects introduced here, highlighted in four different colors. The two equations describe the relationships between these objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae5588",
   "metadata": {},
   "source": [
    "#### Random variables vs probability density functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2dacad",
   "metadata": {},
   "source": [
    "The first concept required to understand this is that of representing random variables (vectors in this case) in terms of their _probability density functions_. Mathematicians do this to enable the analysis of random variables. Although it isn't explicitly mentioned here, $\\pi$, $q$, and $T_\\pi$ are all probability density functions. The pdf $\\pi(\\mathbf y)$ is a function that accepts a vector as input and returns the probability that the provided value of $\\mathbf y$ might occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b98f36",
   "metadata": {},
   "source": [
    "Pdfs have been studied for a very long time, so they have many properties which are now well understood by mathematicians."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ac30a",
   "metadata": {},
   "source": [
    "The authors of this paper use the notation that the input symbol of the pdf is the name of the random vector described by said pdf. For example, $\\mathbf y$ is a _random vector_ whose behavior is completely described by the probability density function $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008ad735",
   "metadata": {},
   "source": [
    "Coders often find it easier to think in terms of the random variables and vectors, however, since they're closer to what gets written into our programs in the end. Let's shift focus to the random vectors introduced here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f6a90",
   "metadata": {},
   "source": [
    "#### Data distribution $q(\\mathbf x^{(0)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88885484",
   "metadata": {},
   "source": [
    "There are a few more subtleties in the notation used for these random vectors. The first random vector we were introduced to is $\\mathbf x^{(0)}$. $x$ is often used for the object that's ultimately being modeled. In this case, it's a random vector representing the _data distribution_. For example, if we were using the MNIST dataset, then $\\mathbf x^{(0)}$ is a randomly sampled image that looks like an MNIST digit. It may not be an actual image from the dataset -- this is the goal of the abstraction \"data distribution\", it assumes some underlying data generating system, of which the actual MNIST dataset is only one sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a5fd4",
   "metadata": {},
   "source": [
    "The superscript $(0)$ here does not mean \"to the power of\". This is suggested by the brackets surrounding the zero, a notation commonly used in some domains including physics. The use of the superscript already suggests that we might see the sequence continue, for example, $\\mathbf x^{(1)}$, $\\mathbf x^{(2)}$, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762ae49",
   "metadata": {},
   "source": [
    "#### Analytically tractable distribution $\\pi(\\mathbf y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f466e",
   "metadata": {},
   "source": [
    "The use of $\\mathbf y$ in $\\pi(\\mathbf y)$ suggests that the authors are trying to highlight a more general property about the functions $\\pi$ and $T_\\pi$ that's totally unrelated to the data $\\mathbf x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1aa10f",
   "metadata": {},
   "source": [
    "What does \"analytically tractable\" distribution mean? I'm not 100% sure but I think it means a relatively straightforward formula that we can type out in Python when defining a simple function. In other words, the authors claim that we can gradually convert the data distribution (some hypothetical Python function that returns novel images that look like MNIST digits) into a relatively straightforward mathematical formula that can be programmed in Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb307ed4",
   "metadata": {},
   "source": [
    "If we peak ahead to Table App. 1[^peaking], we can see some examples of these objects. However, instead of $\\pi(\\mathbf y)$ we now see $\\pi(\\mathbf x^{(T)})$, which may be set to $\\mathcal N (\\mathbf x^{(T)}; \\mathbf 0, \\mathbf I)$ (Gaussian) or $\\mathcal B(\\mathbf x^{(T)}; 0.5)$ (Binomial).\n",
    "\n",
    "This is a new notation, let's describe it. $\\mathcal N$ and $\\mathcal B$ refer to pdfs of two well-known distributions: Gaussian and Binomial. The semicolon separates function inputs on the left from parameters on the right. It's not immediately obvious what the difference is between an input and parameter of a function, they only really make sense for classes of functions aka models. For example, the equation of a Gaussian pdf can be written down as a function of the inputs as well as two parameters. Recall from earlier that this is why we might call it an analytically tractable distribution.\n",
    "\n",
    "[^peaking]: It's often a good idea to peak ahead while reading a scientific paper. In fact, I often find it most useful to jump back and forth many times and very rarely read papers linearly. I was introduced to this and other ideas in the wonderful paper: [\"How to Read a Paper\"](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf) by Srinivasan Keshav."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce54afa",
   "metadata": {},
   "source": [
    "#### Equation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d23a2f",
   "metadata": {},
   "source": [
    "I'm not sure that my interpretation of this equation is correct. Please let me know of any mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9043e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\pi(\\mathbf y) = \\int d\\mathbf y' T_\\pi (\\mathbf y|\\mathbf y';\\beta) \\pi(\\mathbf y') \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84bb5f7",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a818d39",
   "metadata": {},
   "source": [
    "You can think of a _distribution_ as a Python function with a random output. A _data_ distribution is such a random function that returns some example from a given dataset. In _probability theory_, we study the behavior random objects and randomness in general, including these sorts of random functions.\n",
    "\n",
    "$\\mathbf{x}^{(0)}$ represents the image, and $q$ represents the \"distribution\" of images -- which we said we would think of as a function that randomly returns one of the images in our dataset.\n",
    "\n",
    "You might be wondering why $\\mathbf{x}^{(0)}$ is an input to $q$ if it gets returned from our function. That's because in probability theory, we define the distribution as a function that accepts an image (in this case) and returns a number between 0 and 1 that tells us how likely it is that that image would occur in our dataset. This definition turns out to be very useful to study these sorts of random variables. Although it isn't immediately obvious what the likelihood of any given MNIST image is. But we can quite easily sample from the MNIST \"distribution\" without knowing that!\n",
    "\n",
    "For example, here's how we might construct the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44445c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBUlEQVR4nGNgGMRAUBCnlNzcBw+lUYVYYIzydHkGRrP1DAyhHEoMXd+R1bCv+vfv3796TvaOu//+/fv/dhIDAwMDAyMDAwMDA+fc8C+zehheMnCvZjjLIGitxVTWD9fY9PdvG7JB1g/ewNlS7//uRHVKyV8GBgYGJgYGBoYO/nXuaG5nhLOe/o1AkzuO0LmH4RqqnJred7jkfQZnVMlcjh4kBy1GkZv5770kgjfpkzmCIzb535dZSEq1HpwyhjLZNa78veeP5q9LWgwMDAycsav+/r0dguoCzvn/377dtfvfv3//PrYxYICgpV/+/vv793YxUrQhQoJBjYOBgeH+Z0x9gwcAAJmuVkWiZLQCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "paths = list(walk(path))\n",
    "p = random.choice(paths)\n",
    "im = Image.open(p)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c776fb1",
   "metadata": {},
   "source": [
    "Every time you run the cell above, you'll \"sample\" from the MNIST dataset, in other words, you'll sample an image $\\mathbf{x}^{(0)}$ from the distribution $q(\\mathbf{x}^{(0)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6618f38a",
   "metadata": {},
   "source": [
    "Although this isn't quite right! The hypothesis in the paper, and in generative modeling in general, is that this dataset itself is only a sample from some underlying data distribution. We hypothesis that there exists some random function that would return other images that look a lot like MNIST images, but aren't exactly the same -- the underlying data distribution $q$.\n",
    "\n",
    "The goal of generative modeling is to learn how to sample from such a data distribution, starting out from a sample dataset drawn from that distribution. This is a useful mathematical framing, although it might be simpler to say that the goal is to learn how to generate novel images that look similar to images from a given dataset.\n",
    "\n",
    "The tools of probability theory allow us to better understand the problem, and devise potential solutions, by framing it in this slightly weird format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e268cc5",
   "metadata": {},
   "source": [
    "We haven't gotten to what the $(0)$ above the $\\mathbf{x}$ means yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390375f",
   "metadata": {},
   "source": [
    "> The data distribution is gradually converted into a well behaved (analytically tractable) distribution $\\pi(y)$ by repeated application of a Markov diffusion kernel $T_\\pi(y|y′; \\beta)$ for $\\pi(y)$, where $\\beta$ is the diffusion rate,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1bbf57",
   "metadata": {},
   "source": [
    "Okay, there's _tons_ to unpack in this sentence!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dae239",
   "metadata": {},
   "source": [
    "> The data distribution is gradually converted into a well behaved (analytically tractable) distribution $\\pi(y)$ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec385d",
   "metadata": {},
   "source": [
    "This is starting to explaining why there's a $(0)$ above the $\\mathbf{x}$ -- because we'll be gradually converting the data distribution (think MNIST images) into something else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78871ef2",
   "metadata": {},
   "source": [
    "We'll be converting it into a \"well behaved (analytically tractable) distribution\". What does that mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e50454",
   "metadata": {},
   "source": [
    "I'm not too sure what well behaved is supposed to mean here to be honest! However, I think \"analytically tractable\" means a relatively straightforward formula that we can type out in Python when defining a simple function. In other words, the authors claim that we can gradually convert the data distribution (some hypothetical Python function that returns novel images that look like MNIST digits) into a relatively straightforward mathematical formula that can be programmed in Python! If we can do that, we've solved the problem we're trying to solve -- of generative modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0fca3",
   "metadata": {},
   "source": [
    "I don't think this is true just yet. This is only the forward process!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7355abb",
   "metadata": {},
   "source": [
    "They call that final distribution $\\pi(\\mathbf y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ae167",
   "metadata": {},
   "source": [
    "Where do $\\pi$ and $\\mathbf y$ come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e8584",
   "metadata": {},
   "source": [
    "I'm not sure! But I think the intention behind using $\\mathbf y$ instead of something like $\\mathbf x$ is to signify that they're different random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd9445",
   "metadata": {},
   "source": [
    "A \"Markov diffusion kernel\" basically means a function that is iteratively applied to some input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b50c8e",
   "metadata": {},
   "source": [
    "### 2.1 Forward trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0754505d",
   "metadata": {},
   "source": [
    "We label the data distribution $q(\\mathbf x^{(0)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1934b97b",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\pi(\\mathbf y) = \\int d\\mathbf y' T_\\pi (\\mathbf y|\\mathbf y';\\beta) \\pi(\\mathbf y')\n",
    "\\tag{1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec7517a",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "### What does this mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abb6cb",
   "metadata": {},
   "source": [
    "I think the point of this equation is to say that $T_\\pi$ is some \"kernel\" which if convolved with the distribution $\\pi$ gives another distribution of the same functional form. The point of the equation is to tell us how $\\pi$ and $T_\\pi$ relate to each other. I'm not sure why this property is important though... and whether/why it's satisfied by Gaussian and Binomial distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f098ea",
   "metadata": {},
   "source": [
    "I think the point is that applying $T_\\pi$ to $\\pi$ has no effect! Remember that this is about distributions, so the distribution is the same after \"applying\" the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af41ed",
   "metadata": {},
   "source": [
    "Let's reframe this in terms of the forward process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796af46",
   "metadata": {},
   "source": [
    "$$\n",
    "q(\\mathbf x^{(t)}) = \\int d\\mathbf x^{(t-1)} q(\\mathbf x^{(t)}|\\mathbf x^{(t-1)}) q(\\mathbf x^{(t-1)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b54f6",
   "metadata": {},
   "source": [
    "(By the way, this is equivalent to saying the following, due to the law of total probability -- and some other assumptions?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31881d04",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "q(\\mathbf x^{(0\\cdots T)}) = q(\\mathbf x^{(0)}) \\prod_{t=1}^T q(\\mathbf x^{(t)}|\\mathbf x^{(t-1)})\n",
    "\\tag{3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50824fbf",
   "metadata": {},
   "source": [
    "For a Gaussian kernel:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aedf3d",
   "metadata": {},
   "source": [
    "$$\n",
    "q(\\mathbf x^{(t)}) = \\int d\\mathbf x^{(t-1)} \\mathcal N(\\mathbf x^{(t)}; \\mathbf x^{(t-1)}\\sqrt{1-\\beta_t}, \\mathbf I \\beta_t) q(\\mathbf x^{(t-1)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9153e4c",
   "metadata": {},
   "source": [
    "Since its a Gaussian:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d3d80",
   "metadata": {},
   "source": [
    "$$\n",
    "q(\\mathbf x^{(t)}) = \\int d\\mathbf x^{(t-1)} \\mathcal N(\\mathbf x^{(t)} - \\mathbf x^{(t-1)}\\sqrt{1-\\beta_t}; \\mathbf 0, \\mathbf I \\beta_t) q(\\mathbf x^{(t-1)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc536f",
   "metadata": {},
   "source": [
    "Which is the definition of convolution of two functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7c642",
   "metadata": {},
   "source": [
    "$$\n",
    "q(\\mathbf x^{(t)}) = \\mathcal N(\\mathbf x^{(t)}; \\mathbf 0, \\mathbf I \\beta_t) \\ast q(\\mathbf x^{(t-1)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc79470",
   "metadata": {},
   "source": [
    "Which, for independent distributions, is equivalent to adding their random variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad00260",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf x^{(t)} = \\sqrt{\\beta_t} \\epsilon_t + q(\\mathbf x^{(t-1)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaefbd4",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc9c86",
   "metadata": {},
   "source": [
    "$$\n",
    "\\epsilon_t = \\mathcal N(\\mathbf x^{(t)}; \\mathbf 0, \\mathbf I)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c20e3",
   "metadata": {},
   "source": [
    "The equation in terms of probability density functions is useful to work with mathematically, whereas the equation in terms of random variables is much closer to what the final code looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb2d46",
   "metadata": {},
   "source": [
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de390622",
   "metadata": {},
   "source": [
    "Forward diffusion kernel:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1ecf3",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "q(\\mathbf x^{(t)}|\\mathbf x^{(t-1)}) = T_\\pi(\\mathbf x^{(t)} | \\mathbf x^{(t-1)}; \\beta_t)\n",
    "\\tag{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903da0ce",
   "metadata": {},
   "source": [
    "Forward diffusion process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b46b7",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "q(\\mathbf x^{(0\\cdots T)}) = q(\\mathbf x^{(0)}) \\prod_{t=1}^T q(\\mathbf x^{(t)}|\\mathbf x^{(t-1)})\n",
    "\\tag{3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0777fa",
   "metadata": {},
   "source": [
    "### 2.2 Reverse trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1b455",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "p(\\mathbf x^{(T)}) = \\pi(\\mathbf x^{(T)})\n",
    "\\tag{4}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e77a792",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "p(\\mathbf x^{(0\\cdots T)}) = p(\\mathbf x^{(T)}) \\prod_{t=1}^T p(\\mathbf x^{(t-1)}|\\mathbf x^{(t)})\n",
    "\\tag{5}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ddbe8",
   "metadata": {},
   "source": [
    "### Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb358690",
   "metadata": {},
   "source": [
    "Well-behaved (analytically tractable) distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d220c6",
   "metadata": {},
   "source": [
    "$$\\pi(\\mathbf x^{(T)}) = \\mathcal N (\\mathbf x^{(T)}; \\mathbf 0, \\mathbf I)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7196eeb",
   "metadata": {},
   "source": [
    "Forward diffusion kernel:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a4b8ac",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "q(\\mathbf x^{(t)}|\\mathbf x^{(t-1)}) = \\mathcal N(\\mathbf x^{(t)}; \\mathbf x^{(t-1)}\\sqrt{1-\\beta_t}, \\mathbf I \\beta_t)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69afc78f",
   "metadata": {},
   "source": [
    "Reverse diffusion kernel:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc038749",
   "metadata": {},
   "source": [
    "$$p(\\mathbf x^{(t-1)}|\\mathbf x^{(t)}) = \\mathcal N(\\mathbf x^{(t-1)}; \\mathbf f_\\mu(x^{(t)}, t), \\mathbf f_\\Sigma(x^{(t)}, t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced598e2",
   "metadata": {},
   "source": [
    "Training targets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8ffd2",
   "metadata": {},
   "source": [
    "$$\\mathbf f_\\mu(x^{(t)}, t), \\mathbf f_\\Sigma(x^{(t)}, t)), \\beta_{1\\cdots T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752535c",
   "metadata": {},
   "source": [
    "Forward distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3c988",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "q(\\mathbf x^{(0\\cdots T)}) = q(\\mathbf x^{(0)}) \\prod_{t=1}^T q(\\mathbf x^{(t)}|\\mathbf x^{(t-1)})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f725050",
   "metadata": {},
   "source": [
    "Reverse distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668232b",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "p(\\mathbf x^{(0\\cdots T)}) = \\pi(\\mathbf x^{(T)}) \\prod_{t=1}^T p(\\mathbf x^{(t-1)}|\\mathbf x^{(t)})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670ba62",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59c528",
   "metadata": {},
   "source": [
    "I think the first and second mean:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552ae2e",
   "metadata": {},
   "source": [
    "$$\\pi(\\mathbf x^{(T)}) = \\mathcal N (\\mathbf x^{(T)}; \\mathbf 0, \\mathbf I) = \\int d\\mathbf ? T_\\pi (\\mathbf x^{(T)}|\\mathbf ?;\\beta) \\pi(\\mathbf ?)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831394a7",
   "metadata": {},
   "source": [
    "$$\\pi(\\mathbf x^{(T)}) = \\mathcal N (\\mathbf x^{(T)}; \\mathbf 0, \\mathbf I) = \\int d\\mathbf x \\mathcal N(\\mathbf x^{(T)}; \\mathbf x \\sqrt{1-\\beta_t}, \\mathbf I \\beta_t) \\pi(\\mathbf x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa8627",
   "metadata": {},
   "source": [
    "I have _no idea_ what the integral means!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2849fa",
   "metadata": {},
   "source": [
    "## Discovering diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a9532",
   "metadata": {},
   "source": [
    "We define a \"forward\" process, with known parameters, that starts from our data distribution (actual images) and applies a series of very tiny corruptions until we eventually reach pure noise.\n",
    "\n",
    "If we do this within some constraints (Markov process, Gaussian transitions, Gaussian noise at the end, small step sizes), then the reverse process will have the same functional form (Markov process, Gaussian transitions, Gaussian noise at the beginning, small step sizes)! The only part that we don't yet know is the parameters of this reverse process.\n",
    "\n",
    "The go-to method of estimating parameters of a probabilistic model is to find the parameters that maximize the log likelihood function. While we can do that with closed-form solutions that we can write down for simple distributions, it becomes impossible for more complex distributions. And it can't be computed because it would require integrating over a high-dimensional and continuous space. Instead, there's a trick to optimize a lower bound of it, called the \"ELBO\", which in practice is just as good but quite a lot easier to calculate.\n",
    "\n",
    "The original paper worked through the math to figure out what the ELBO is if we estimate the mean and variance of the reverse diffusion kernel.\n",
    "\n",
    "DDPM improved on that by "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb6a0d",
   "metadata": {},
   "source": [
    "Start by assuming that your data was generated by a Markov process starting from pure random noise (this has really convenient mathematical properties) and gradually refining it.\n",
    "\n",
    "If the refinement is tiny at each step, then there's some math that says that the forward and reverse process have the same functional form. Meaning that we should be able to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
